{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Auto-find project root (works across platforms)\n",
    "project_root = Path.cwd().parent.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Looking for .wav files in: /Users/user/Documents/Inno/GenAI/VoiceCloning/voice_cloning/speaker_encoder/data/output/sample\n",
      "Found 1993 audio files\n",
      "Found 819 unique speakers\n",
      "Creating dataset with 1993 files and 819 speakers\n",
      "Number of speakers in dataset: 819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Documents/Inno/GenAI/VoiceCloning/.venv/lib/python3.9/site-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (1024) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n",
      "Evaluating: 100%|██████████| 1000/1000 [01:31<00:00, 10.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy at threshold 0.9: 0.6490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [01:20<00:00, 12.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy at threshold 0.95: 0.5450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "\n",
    "\n",
    "# Auto-find project root (works across platforms)\n",
    "if str(Path.cwd().parent) not in sys.path:\n",
    "    sys.path.append(str(Path.cwd().parent))\n",
    "    \n",
    "# Assume ECAPA_TDNN model code is imported or defined here\n",
    "    # This is the correct way, assuming project root is in sys.path\n",
    "from voice_cloning.speaker_encoder.ecapa_tdnn import ECAPA_TDNN_SMALL\n",
    "\n",
    "class GE2ELoss(nn.Module):\n",
    "    def __init__(self, init_w=10.0, init_b=-5.0):\n",
    "        super(GE2ELoss, self).__init__()\n",
    "        self.w = nn.Parameter(torch.tensor(init_w))\n",
    "        self.b = nn.Parameter(torch.tensor(init_b))\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        N, M, D = embeddings.shape\n",
    "        \n",
    "        centroids = torch.mean(embeddings, dim=1)\n",
    "        sum_centroids = centroids * M\n",
    "        sum_centroids_excl = sum_centroids.unsqueeze(1) - embeddings\n",
    "        centroids_excl = sum_centroids_excl / (M - 1 + 1e-6)\n",
    "        \n",
    "        embeddings_flat = embeddings.reshape(N*M, D)\n",
    "        centroids_excl_flat = centroids_excl.reshape(N*M, D)\n",
    "        \n",
    "        sim_matrix = F.cosine_similarity(\n",
    "            embeddings_flat.unsqueeze(1), \n",
    "            centroids.unsqueeze(0), \n",
    "            dim=2\n",
    "        )\n",
    "        \n",
    "        sim_self = F.cosine_similarity(embeddings_flat, centroids_excl_flat, dim=1)\n",
    "        \n",
    "        speaker_indices = torch.arange(N).view(N, 1).expand(-1, M).reshape(N*M)\n",
    "        sim_matrix[torch.arange(N*M), speaker_indices] = sim_self\n",
    "        \n",
    "        sim_matrix = sim_matrix * self.w + self.b\n",
    "        loss = F.cross_entropy(sim_matrix, speaker_indices.to(embeddings.device))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "class GE2EBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, n_speakers, n_utterances, num_batches):\n",
    "        self.n_speakers = n_speakers\n",
    "        self.n_utterances = n_utterances\n",
    "        self.num_batches = num_batches\n",
    "        \n",
    "        self.speaker_to_indices = defaultdict(list)\n",
    "        for idx, (_, spk_id) in enumerate(dataset):\n",
    "            self.speaker_to_indices[spk_id].append(idx)\n",
    "        \n",
    "        self.speakers = list(self.speaker_to_indices.keys())\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for _ in range(self.num_batches):\n",
    "            selected_speakers = np.random.choice(\n",
    "                self.speakers, self.n_speakers, replace=False\n",
    "            )\n",
    "            batch = []\n",
    "            for speaker in selected_speakers:\n",
    "                indices = self.speaker_to_indices[speaker]\n",
    "                if len(indices) < self.n_utterances:\n",
    "                    selected = np.random.choice(\n",
    "                        indices, self.n_utterances, replace=True\n",
    "                    )\n",
    "                else:\n",
    "                    selected = np.random.choice(\n",
    "                        indices, self.n_utterances, replace=False\n",
    "                    )\n",
    "                batch.extend(selected)\n",
    "            yield batch\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "class VoxCeleb2Dataset(Dataset):\n",
    "    def __init__(self, audio_paths, speaker_ids, sr=16000, duration=3):\n",
    "        self.sr = sr\n",
    "        self.duration = duration\n",
    "        self.audio_paths = audio_paths\n",
    "        self.speaker_ids = speaker_ids\n",
    "        self.spk_to_id = {spk: idx for idx, spk in enumerate(set(speaker_ids))}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def load_audio(self, path: str):\n",
    "        wav_ref, sr = librosa.load(path)\n",
    "        wav_ref = torch.FloatTensor(wav_ref).unsqueeze(0)\n",
    "        resample_fn = torchaudio.transforms.Resample(sr, self.sr)\n",
    "        wav_ref = resample_fn(wav_ref)\n",
    "        return wav_ref\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio and process to fixed length\n",
    "        waveform = self.load_audio(self.audio_paths[idx])  # Implement this\n",
    "        waveform = self.process_waveform(waveform)\n",
    "        speaker_id = self.spk_to_id[self.speaker_ids[idx]]\n",
    "        return waveform, speaker_id\n",
    "    \n",
    "    def process_waveform(self, waveform):\n",
    "        target_len = self.sr * self.duration\n",
    "        if waveform.shape[-1] > target_len:\n",
    "            start = np.random.randint(0, waveform.shape[-1] - target_len)\n",
    "            waveform = waveform[..., start:start+target_len]\n",
    "        else:\n",
    "            pad = target_len - waveform.shape[-1]\n",
    "            waveform = F.pad(waveform, (0, pad))\n",
    "        return waveform.squeeze()\n",
    "\n",
    "def collate_fn(batch):\n",
    "    waveforms, speaker_ids = zip(*batch)\n",
    "    waveforms = torch.stack(waveforms)\n",
    "    speaker_ids = torch.LongTensor(speaker_ids)\n",
    "    return waveforms, speaker_ids\n",
    "\n",
    "# Training Configuration\n",
    "n_speakers = 4      # Reduced from 5 to handle CPU memory better\n",
    "n_utterances = 4    # Reduced from 5 to handle CPU memory better\n",
    "batch_size = n_speakers * n_utterances  # = 16 utterances per batch\n",
    "# Calculate a reasonable number of batches per epoch\n",
    "# With 4874 recordings, let's aim to see each recording roughly once per epoch\n",
    "num_batches = 4874 // batch_size  # ≈ 304 batches\n",
    "emb_dim = 256\n",
    "lr = 1e-5\n",
    "num_epochs = 60\n",
    "\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "# el\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "folder_path = \"./data/output/sample\"\n",
    "\n",
    "def get_audio_paths_and_speaker_ids(vox1_test_wav_folder):\n",
    "    audio_paths = []\n",
    "    speaker_ids = []\n",
    "\n",
    "    # Add debug prints\n",
    "    print(f\"Looking for .wav files in: {os.path.abspath(vox1_test_wav_folder)}\")\n",
    "\n",
    "    # Traverse the directory structure\n",
    "    for root, dirs, files in os.walk(vox1_test_wav_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                # Full path to the .wav file\n",
    "                audio_paths.append(os.path.join(root, file))\n",
    "                \n",
    "                # Extract speaker ID from the path\n",
    "                speaker_id = os.path.normpath(root).split(os.sep)[-2]\n",
    "                speaker_ids.append(speaker_id)\n",
    "\n",
    "    # Add debug prints\n",
    "    print(f\"Found {len(audio_paths)} audio files\")\n",
    "    print(f\"Found {len(set(speaker_ids))} unique speakers\")\n",
    "    \n",
    "    if len(audio_paths) == 0:\n",
    "        raise ValueError(f\"No .wav files found in {vox1_test_wav_folder}\")\n",
    "\n",
    "    return audio_paths, speaker_ids\n",
    "\n",
    "def evaluate(model, test_csv_path, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    test_df = pd.read_csv(test_csv_path).sample(n=1000, random_state=42)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Evaluating\"):\n",
    "            # Load both audio files\n",
    "            audio1_path = os.path.join(folder_path, row['audio_1'])\n",
    "            audio2_path = os.path.join(folder_path, row['audio_2'])\n",
    "            \n",
    "            # Create dataset instances for single files\n",
    "            dataset = VoxCeleb2Dataset([audio1_path, audio2_path], ['spk1', 'spk2'])\n",
    "            \n",
    "            # Get embeddings\n",
    "            audio1, _ = dataset[0]\n",
    "            audio2, _ = dataset[1]\n",
    "            \n",
    "            audio1 = audio1.unsqueeze(0).to(device)\n",
    "            audio2 = audio2.unsqueeze(0).to(device)\n",
    "            \n",
    "            emb1 = model(audio1)\n",
    "            emb2 = model(audio2)\n",
    "            \n",
    "            # Calculate similarity\n",
    "            similarity = F.cosine_similarity(emb1, emb2)\n",
    "            \n",
    "            # Predict (similarity > 0.5 indicates same speaker)\n",
    "            prediction = (similarity > threshold).int().item()\n",
    "            \n",
    "            # Compare with ground truth\n",
    "            correct += (prediction == row['label'])\n",
    "            total += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Make folder path absolute if it's relative\n",
    "folder_path = os.path.abspath(folder_path)\n",
    "\n",
    "audio_paths, speaker_ids = get_audio_paths_and_speaker_ids(folder_path)\n",
    "\n",
    "# Add debug print before creating dataset\n",
    "print(f\"Creating dataset with {len(audio_paths)} files and {len(set(speaker_ids))} speakers\")\n",
    "\n",
    "train_dataset = VoxCeleb2Dataset(audio_paths, speaker_ids)\n",
    "\n",
    "# Add debug print for batch sampler\n",
    "print(f\"Number of speakers in dataset: {len(train_dataset.spk_to_id)}\")\n",
    "\n",
    "batch_sampler = GE2EBatchSampler(\n",
    "    train_dataset, \n",
    "    n_speakers=min(n_speakers, len(train_dataset.spk_to_id)),  # Ensure n_speakers isn't larger than available speakers\n",
    "    n_utterances=n_utterances, \n",
    "    num_batches=num_batches\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_sampler=batch_sampler, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "resume_checkpoint = \"../../checkpoints/speaker_encoder.pt\"\n",
    "model = ECAPA_TDNN_SMALL(\n",
    "    feat_dim=1024,\n",
    "    feat_type=\"fbank\",\n",
    ")\n",
    "\n",
    "state_dict = torch.load(resume_checkpoint, map_location=lambda storage, loc: storage)\n",
    "model.load_state_dict(state_dict['model'], strict=False)\n",
    "_ = model.eval()\n",
    "\n",
    "criterion = GE2ELoss().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "test_csv_path = \"./data/output/test.csv\"\n",
    "\n",
    "for threshold in [0.9, 0.95]:\n",
    "    accuracy = evaluate(model, test_csv_path, device, threshold=threshold)\n",
    "    print(f\"Evaluation Accuracy at threshold {threshold}: {accuracy:.4f}\")\n",
    "\n",
    "# Training Loop\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
